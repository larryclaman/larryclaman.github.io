<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="generator" content="Hugo 0.89.2" />
		<title>KEDA, Windows, and Batch Jobs - Larry Claman&#39;s Blog</title>

		<meta name="description" content="Sample using KEDA to schedule Windows jobs as part of a batch processing workload">


		
	
		




<link rel="stylesheet" href="/css/ui.css">

	
		

		<link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono|Lato|Raleway">

		
		<script type="text/javascript">
			(function(c,l,a,r,i,t,y){
				c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
				t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
				y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
			})(window, document, "clarity", "script", "44j7jjbeip");
		</script>
			

		
	</head>

<body>
<header class="container no-print">
	<div class="u-header">
		<nav class="bar">
	<ul><li>
			<a href="/">
				<img class="icon-text" src="/img/prev.svg"/>
			</a>
		</li><li><a href="/">Blog</a></li></ul>
</nav>

	</div>
</header>
<main class="container">

<article>
	<header><hgroup id="brand">
	<h1>KEDA, Windows, and Batch Jobs</h1>
	<h5>
		
		<time datetime="2021-11-14 18:33:31 -0500 -0500">Nov 14, 2021</time>
		<span class="no-print">
			-
				
				<a href="/tags/azure">azure</a>
				
				<a href="/tags/aks">aks</a>
				
				<a href="/tags/kubernetes">kubernetes</a>
				
				<a href="/tags/keda">keda</a>
				<span>
	</h5>
	
</hgroup>
<hr class="sep" />
</header>
	<p>I finally had a reason to dive into <a href="https://keda.sh">KEDA</a> in conjunction with AKS and learned a lot.  I&rsquo;ve been working with a customer who&rsquo;s been looking to migrate a homegrown, on-premises batch process to Azure Kubernetes Service.  KEDA, <em>Kubernetes Event Driven Autoscaling</em>, looked to be a great way to solve this challenge, but there were some requirements that made the answer not-so-simple.  The first challenge was, can it work with Windows workloads?</p>
<p>The answer to the first question is, <strong>YES</strong>!  Let&rsquo;s walk through this:</p>
<h2 id="keda--windows">KEDA &amp; Windows</h2>
<p>I started with the excellent samples at <a href="https://github.com/tomconte/sample-keda-queue-jobs">https://github.com/tomconte/sample-keda-queue-jobs</a>, but I needed to take these and substantially update them 1) to use KEDA 2.0, and 2) to support Windows workers.</p>
<p>These samples use an illustrative application that places message (eg, &lsquo;jobs&rsquo;) in an Azure Storage queue.  KEDA is then used to schedule Kubernetes jobs using this queue as a trigger.  Also notice we are scheduling <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Kubernetes jobs</a>, not deployments; these work extremely well for this use case but are a little less common than deployments.</p>
<h3 id="aks-cluster--installing-keda">AKS Cluster &amp; installing KEDA</h3>
<p>We&rsquo;ll start by creating the AKS cluster:</p>
<pre tabindex="0"><code>RG=keda-sample
LOCATION=westus2
CLUSTER_NAME=&quot;kedatest&quot;
az group create -l $LOCATION -n $RG

az aks create \
 -g $RG \
 -n $CLUSTER_NAME \
 --node-count 1 \
 --node-vm-size Standard_DS3_v2 \
 --generate-ssh-keys \
  --node-osdisk-type Managed \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 10 \
  --network-plugin azure 

az aks get-credentials -g $RG -n $CLUSTER_NAME
</code></pre><p>One of the things we&rsquo;re going to do for this use case is use the new AKS <a href="https://docs.microsoft.com/en-us/azure/aks/scale-down-mode">deallocate</a> scale down mode to improve scale out performance.</p>
<pre tabindex="0"><code>az aks nodepool update --scale-down-mode Deallocate --name nodepool1  --cluster-name $CLUSTER_NAME --resource-group $RG
</code></pre><p>Let&rsquo;s install KEDA:</p>
<pre tabindex="0"><code>helm repo add kedacore https://kedacore.github.io/charts
helm repo update
kubectl create namespace keda
helm install keda kedacore/keda --version 2.4.0 --namespace keda
</code></pre><p>And finally, let&rsquo;s add a Windows node pool:</p>
<pre tabindex="0"><code>az aks nodepool add -g $RG --cluster-name $CLUSTER_NAME -n win1  --os-type Windows \
   --enable-cluster-autoscaler --min-count 0 --max-count 10 --scale-down-mode Deallocate
</code></pre><h3 id="storage-queue">Storage Queue</h3>
<p>Now we need to set up the storage queue:</p>
<pre tabindex="0"><code>STORAGE_ACCOUNT_NAME=kedademo # set to your storage acct name; must be globally unique
export QUEUE_NAME=keda-queue
az storage account create -g $RG -n $STORAGE_ACCOUNT_NAME
az storage queue create -n $QUEUE_NAME
export AzureWebJobsStorage=$(az storage account show-connection-string --name $STORAGE_ACCOUNT_NAME --query connectionString -o tsv)  # this env variable will be used later by our application

# save the storage account connection string as a Kubernetes Secret
kubectl create secret generic secrets \
    --from-literal=AzureWebJobsStorage=$AzureWebJobsStorage
</code></pre><h3 id="worker-application">Worker application</h3>
<p>Next, we need to build a Windows container image with our worker application. I&rsquo;ll use Azure Container registry to do the actual image build.  I&rsquo;ll also share the source code for the application at the end of this blog post.</p>
<pre tabindex="0"><code>export ACR=kedatest01  # set to your ACR name; must be globally unique
az acr create -n $ACR -g $RG --sku Standard
az aks update -n $CLUSTER_NAME -g $RG  --attach-acr $ACR

az acr build -r $ACR -t $ACR.azurecr.io/queue-consumer-windows  --platform windows queue-consumer-windows
</code></pre><h2 id="running-the-sample">Running the sample</h2>
<p>We&rsquo;re now ready to start using our sample application.  As I mentioned above, we&rsquo;re going to use a KEDA ScaledJob to handle scaling out workers.  Here&rsquo;s the Kubernetes manifest: (which is a file named <em>azurequeue_scaledobject_jobs_windows.yaml</em>)</p>
<pre tabindex="0"><code>apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: azure-queue-scaledobject-jobs-win
  namespace: default
spec:
  pollingInterval: 30
  maxReplicaCount: 50
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTargetRef:
    parallelism: 1
    completions: 1
    activeDeadlineSeconds: 600
    backoffLimit: 6
    template:
      spec:
        nodeSelector:
          kubernetes.io/os: windows
        containers:
        - name: consumer-job
          image: $ACR.azurecr.io/queue-consumer-windows
          resources:
            requests:
              cpu: 100m
              memory: 2000Mi # intentionally set high in order to trigger cluster autoscaler
            limits:
              cpu: 100m
              memory: 2000Mi
          env:
          - name: AzureWebJobsStorage
            valueFrom:
              secretKeyRef:
                name: secrets
                key: AzureWebJobsStorage
          - name: QUEUE_NAME
            value: keda-queue
  triggers:
  - type: azure-queue
    metadata:
      queueName: keda-queue
      queueLength: '1'
      connectionFromEnv: AzureWebJobsStorage
</code></pre><p>Deploy this as follows:</p>
<pre tabindex="0"><code>cat azurequeue_scaledobject_jobs_windows.yaml| envsubst | kubectl apply -f -
</code></pre><p>And finally, run the python app to load up the queue with work:</p>
<pre tabindex="0"><code>python3 -m venv .venv # only run once
source .venv/bin/activate
pip install -r requirements.txt # only run once

python send_messages.py 100 # adds 100 messages into the queue
</code></pre><p>At this point, KEDA will create up to <em>MaxReplicaCount</em> jobs (50, per our configuration) to process the messages which have been placed in the storage queue.  As each job will consume 2Gb of memory, very quickly the jobs will not be schedulable, and the Cluster Autoscaler will kick in to scale out the cluster.  As jobs complete, KEDA will schedule new ones until it runs out of work (eg, the queue is empty)</p>
<p>In the next blog post, we&rsquo;ll cover some additional tweaking that can be added to this configuration in support of long running jobs.</p>
<h2 id="code-samples">Code Samples</h2>
<h3 id="queue-consumer">Queue Consumer</h3>
<pre tabindex="0"><code>import os
import time
from azure.storage.queue import QueueClient

try:
  connection_string = os.environ['AzureWebJobsStorage']
  queue_name = os.environ['QUEUE_NAME']
except KeyError:
  print('Error: missing environment variable AzureWebJobsStorage or QUEUE_NAME')
  exit(1)

queue = QueueClient.from_connection_string(conn_str=connection_string, queue_name=queue_name)

# Get a single message
message = next(queue.receive_messages())

# Print the message
print(message)

# Delete message from the queue
queue.delete_message(message)

# Sleep for a while, simulating a long-running job
time.sleep(30)
</code></pre><h3 id="queue-consumer-dockerfile">Queue Consumer Dockerfile</h3>
<pre tabindex="0"><code># 1809 required for AKS
FROM python:windowsservercore-1809
WORKDIR /app
RUN pip install azure-storage-queue
COPY queue_consumer.py /app
CMD [&quot;python&quot;, &quot;queue_consumer.py&quot;]
</code></pre><h3 id="send_messagespy">send_messages.py</h3>
<pre tabindex="0"><code>import os
import sys
import time
from azure.storage.queue import QueueClient

try:
  connection_string = os.environ['AzureWebJobsStorage']
  queue_name = os.environ['QUEUE_NAME']
except KeyError:
  print('Error: missing environment variable AzureWebJobsStorage or QUEUE_NAME')
  exit(1)

queue = QueueClient.from_connection_string(conn_str=connection_string, queue_name=queue_name)

for message in range(0, int(sys.argv[1])):
  queue.send_message(content='foo_'+str(message))

</code></pre>
</article>
<nav class="no-print post-nav">

	<a class="prev-post" href="https://larryclaman.github.io/post/2021-08-19-21-bicep-uami/">
		<img class="icon-text" src="/img/prev.svg"/>Azure Bicep &amp; User Assigned Managed Identity</a>


</nav>


<section id="related">
  <h4>See Also</h4>
  <ul>
    
  	<li><a href="/post/2021-04-04-aks-dns-naming/">Easily create DNS names with Azure Kubernetes Service</a></li>
  	
  	<li><a href="/post/2021-08-19-21-bicep-uami/">Azure Bicep &amp; User Assigned Managed Identity</a></li>
  	
  	<li><a href="/post/2020-04-14-azure-spot/">Converting an Azure VM to a Spot type</a></li>
  	
  </ul>
</section>




	<div id="disqus_thread" class="no-print"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'larryclamanblog';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


			<hr class="sep" />
		</main>
		<footer class="container no-print">
			<div class="u-footer">
				

<a href="https://github.com/larryclaman/"><img class="icon-social" src="/img/github.svg" alt="Github"/></a>


<a href="https://larryclaman.github.io/index.xml" target="_blank"><img class="icon-social" src="/img/feed.svg" alt="Feed"></a>

				<p>
					
					
					
				</p>
				
				<a href="#brand">
					<img class="icon-text" src="/img/toup.svg" alt="To Up"/>
					<span>Back to Top</span>
				</a>
				
			</div>
		</footer>
		
	

</body>
</html>

