<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Larry Claman&#39;s Blog</title>
    <link>https://larryclaman.github.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Larry Claman&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Nov 2021 18:33:31 -0500</lastBuildDate>
    
	<atom:link href="https://larryclaman.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KEDA, Windows, and Batch Jobs</title>
      <link>https://larryclaman.github.io/post/2021-11-14-21-keda/</link>
      <pubDate>Sun, 14 Nov 2021 18:33:31 -0500</pubDate>
      
      <guid>https://larryclaman.github.io/post/2021-11-14-21-keda/</guid>
      <description>&lt;p&gt;I finally had a reason to dive into &lt;a href=&#34;https://keda.sh&#34;&gt;KEDA&lt;/a&gt; in conjunction with AKS and learned a lot.  I&amp;rsquo;ve been working with a customer who&amp;rsquo;s been looking to migrate a homegrown, on-premises batch process to Azure Kubernetes Service.  KEDA, &lt;em&gt;Kubernetes Event Driven Autoscaling&lt;/em&gt;, looked to be a great way to solve this challenge, but there were some requirements that made the answer not-so-simple.  The first challenge was, can it work with Windows workloads?&lt;/p&gt;
&lt;p&gt;The answer to the first question is, &lt;strong&gt;YES&lt;/strong&gt;!  Let&amp;rsquo;s walk through this:&lt;/p&gt;
&lt;h2 id=&#34;keda--windows&#34;&gt;KEDA &amp;amp; Windows&lt;/h2&gt;
&lt;p&gt;I started with the excellent samples at &lt;a href=&#34;https://github.com/tomconte/sample-keda-queue-jobs&#34;&gt;https://github.com/tomconte/sample-keda-queue-jobs&lt;/a&gt;, but I needed to take these and substantially update them 1) to use KEDA 2.0, and 2) to support Windows workers.&lt;/p&gt;
&lt;p&gt;These samples use an illustrative application that places message (eg, &amp;lsquo;jobs&amp;rsquo;) in an Azure Storage queue.  KEDA is then used to schedule Kubernetes jobs using this queue as a trigger.  Also notice we are scheduling &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/&#34;&gt;Kubernetes jobs&lt;/a&gt;, not deployments; these work extremely well for this use case but are a little less common than deployments.&lt;/p&gt;
&lt;h3 id=&#34;aks-cluster--installing-keda&#34;&gt;AKS Cluster &amp;amp; installing KEDA&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll start by creating the AKS cluster:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;RG=keda-sample
LOCATION=westus2
CLUSTER_NAME=&amp;quot;kedatest&amp;quot;
az group create -l $LOCATION -n $RG

az aks create \
 -g $RG \
 -n $CLUSTER_NAME \
 --node-count 1 \
 --node-vm-size Standard_DS3_v2 \
 --generate-ssh-keys \
  --node-osdisk-type Managed \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 10 \
  --network-plugin azure 

az aks get-credentials -g $RG -n $CLUSTER_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;One of the things we&amp;rsquo;re going to do for this use case is use the new AKS &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/scale-down-mode&#34;&gt;deallocate&lt;/a&gt; scale down mode to improve scale out performance.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;az aks nodepool update --scale-down-mode Deallocate --name nodepool1  --cluster-name $CLUSTER_NAME --resource-group $RG
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;rsquo;s install KEDA:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;helm repo add kedacore https://kedacore.github.io/charts
helm repo update
kubectl create namespace keda
helm install keda kedacore/keda --version 2.4.0 --namespace keda
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And finally, let&amp;rsquo;s add a Windows node pool:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;az aks nodepool add -g $RG --cluster-name $CLUSTER_NAME -n win1  --os-type Windows \
   --enable-cluster-autoscaler --min-count 0 --max-count 10 --scale-down-mode Deallocate
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;storage-queue&#34;&gt;Storage Queue&lt;/h3&gt;
&lt;p&gt;Now we need to set up the storage queue:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;STORAGE_ACCOUNT_NAME=kedademo # set to your storage acct name; must be globally unique
export QUEUE_NAME=keda-queue
az storage account create -g $RG -n $STORAGE_ACCOUNT_NAME
az storage queue create -n $QUEUE_NAME
export AzureWebJobsStorage=$(az storage account show-connection-string --name $STORAGE_ACCOUNT_NAME --query connectionString -o tsv)  # this env variable will be used later by our application

# save the storage account connection string as a Kubernetes Secret
kubectl create secret generic secrets \
    --from-literal=AzureWebJobsStorage=$AzureWebJobsStorage
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;worker-application&#34;&gt;Worker application&lt;/h3&gt;
&lt;p&gt;Next, we need to build a Windows container image with our worker application. I&amp;rsquo;ll use Azure Container registry to do the actual image build.  I&amp;rsquo;ll also share the source code for the application at the end of this blog post.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;export ACR=kedatest01  # set to your ACR name; must be globally unique
az acr create -n $ACR -g $RG --sku Standard
az aks update -n $CLUSTER_NAME -g $RG  --attach-acr $ACR

az acr build -r $ACR -t $ACR.azurecr.io/queue-consumer-windows  --platform windows queue-consumer-windows
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;running-the-sample&#34;&gt;Running the sample&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re now ready to start using our sample application.  As I mentioned above, we&amp;rsquo;re going to use a KEDA ScaledJob to handle scaling out workers.  Here&amp;rsquo;s the Kubernetes manifest: (which is a file named &lt;em&gt;azurequeue_scaledobject_jobs_windows.yaml&lt;/em&gt;)&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: azure-queue-scaledobject-jobs-win
  namespace: default
spec:
  pollingInterval: 30
  maxReplicaCount: 50
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTargetRef:
    parallelism: 1
    completions: 1
    activeDeadlineSeconds: 600
    backoffLimit: 6
    template:
      spec:
        nodeSelector:
          kubernetes.io/os: windows
        containers:
        - name: consumer-job
          image: $ACR.azurecr.io/queue-consumer-windows
          resources:
            requests:
              cpu: 100m
              memory: 2000Mi # intentionally set high in order to trigger cluster autoscaler
            limits:
              cpu: 100m
              memory: 2000Mi
          env:
          - name: AzureWebJobsStorage
            valueFrom:
              secretKeyRef:
                name: secrets
                key: AzureWebJobsStorage
          - name: QUEUE_NAME
            value: keda-queue
  triggers:
  - type: azure-queue
    metadata:
      queueName: keda-queue
      queueLength: &#39;1&#39;
      connectionFromEnv: AzureWebJobsStorage
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Deploy this as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat azurequeue_scaledobject_jobs_windows.yaml| envsubst | kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice in the above that I&amp;rsquo;m using the tool &lt;code&gt;envsubst&lt;/code&gt; to dynamically insert the value of the $ACR environment variable into the manifest.&lt;/p&gt;
&lt;p&gt;And finally, run the python app to load up the queue with work (100 messages):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;python3 -m venv .venv # only run once
source .venv/bin/activate
pip install -r requirements.txt # only run once

python send_messages.py 100 # adds 100 messages into the queue
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this point, KEDA will create up to &lt;em&gt;MaxReplicaCount&lt;/em&gt; jobs (50, per our configuration) to process the messages which have been placed in the storage queue.  As each job will consume 2Gb of memory, very quickly the jobs will not be schedulable, and the Cluster Autoscaler will kick in to scale out the cluster.  As jobs complete, KEDA will schedule new ones until it runs out of work (eg, the queue is empty).  You can watch jobs being automatically scheduled using the command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get jobs
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the next &lt;a href=&#34;2021-11-19-21-keda2.md&#34;&gt;blog post&lt;/a&gt;, we&amp;rsquo;ll cover some additional tweaking that can be added to this configuration to support long running jobs.&lt;/p&gt;
&lt;h2 id=&#34;code-samples&#34;&gt;Code Samples&lt;/h2&gt;
&lt;h3 id=&#34;queue-consumer&#34;&gt;Queue Consumer&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import os
import time
from azure.storage.queue import QueueClient

try:
  connection_string = os.environ[&#39;AzureWebJobsStorage&#39;]
  queue_name = os.environ[&#39;QUEUE_NAME&#39;]
except KeyError:
  print(&#39;Error: missing environment variable AzureWebJobsStorage or QUEUE_NAME&#39;)
  exit(1)

queue = QueueClient.from_connection_string(conn_str=connection_string, queue_name=queue_name)

# Get a single message
message = next(queue.receive_messages())

# Print the message
print(message)

# Delete message from the queue
queue.delete_message(message)

# Sleep for a while, simulating a long-running job
time.sleep(30)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;queue-consumer-dockerfile&#34;&gt;Queue Consumer Dockerfile&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 1809 required for AKS
FROM python:windowsservercore-1809
WORKDIR /app
RUN pip install azure-storage-queue
COPY queue_consumer.py /app
CMD [&amp;quot;python&amp;quot;, &amp;quot;queue_consumer.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;send_messagespy&#34;&gt;send_messages.py&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import os
import sys
import time
from azure.storage.queue import QueueClient

try:
  connection_string = os.environ[&#39;AzureWebJobsStorage&#39;]
  queue_name = os.environ[&#39;QUEUE_NAME&#39;]
except KeyError:
  print(&#39;Error: missing environment variable AzureWebJobsStorage or QUEUE_NAME&#39;)
  exit(1)

queue = QueueClient.from_connection_string(conn_str=connection_string, queue_name=queue_name)

for message in range(0, int(sys.argv[1])):
  queue.send_message(content=&#39;foo_&#39;+str(message))

&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
